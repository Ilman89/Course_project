Инфраструктура
Terraform - разворачивание необходимой инфраструктуры в YC
•	на текущий момент разворачивается отдельный инстанс с Gitlab, контейнер для хранения Docker-образов, кластер k8s и группа из 3 worker-нод.
•	установка Gitlab производится по скрипту gitlab.sh через провижионер 
•	последующая настройка Gitlab производится через веб-интерфейс
•	установка кластера и воркеров вынесены в модули, разворачиваются согласно документации YC
Установка
•	настроить yc (если это еще не сделано)
•	в каталоге terrafrom выполнить: terraform init
•	Создать копию файла с входными переменными cp terraform.tfvars.example terrafrom.tfvars
•	Актуализируем значения в файле переменных (описание см в самих переменных)
•	Для деплоя запустить deploy_infra.sh
•	Все реквизиты для подключения к сервисам terraform выведет в output
Описание terraform:
•	main.tf - основной файл
•	gitlab-ci-ins – инстанс для Gitlab-CI
•	container-registry – разворачивание хранилища для Docker-контейнеров
•	cluster - разворачивание кластера k8s (в зависимостях у него модуль для разворачивания node-groups)

Деплой приложения 
Краткое описание всего процесса:
•	весь проект состоит из приложения и описания инфраструктуры
•	Предполагается что сначала поднимается инфра и дальше в ней разворачивается приложение
•	Для деплоя приложения используется репозитарий развернутого интсанса Gitlab-CI: http://62.84.117.53/root/course-project 
Структура репа:
•	Папки crawler, ui – сервисы, возможен ручной запуск через docker-compose 
•	k8s.yaml – манифест с полным деплоем приложения для k8s 
•	Grafana.sh – дашборды Grafana
•	gitlab-ci.yaml - Центральный файл гитлаба для деплоя всего релиза в k8s
•	monitoring.sh – установка Prometheus с помощью helm
Автоматическая сборка:
•	разворачивается инфраструктура или используется имеющаяся
•	После разворачивания инфраструктуры, необходимо по полученным реквизитам Gitlab-ins создать в нем 3 job.
•	Первый job - build_crawler сборка поискового движка .
•	Второй - build_ui сборка веба.
•	На третьем – деплой в кластер. Разворачиваются очередь RabbitMQ, база Mongo и само приложение. Для всех создаются абстракции service, типа clusterip, а для ui, service типа loadbalancer, для возможности доступа снаружи. В дальнейшем планируется перейти на ingress controller.
•	Адрес приложения развернутого в кластере: http://178.154.222.123:8000/ 
Мониторинг
•	Для сбора метрик k8s (используются деофолтные настройки последней версии community чарта) устанавливается Prometheus через скрипт monitoring.sh. Для этого используется helm. У service my-prometheus-server тип меняется на LoadBalancer, для доступа снаружи.
•	Адрес Prometheus : http://84.201.173.7/graph 
•	datasource и общедоступные дашборды для общих метрик k8s устанавливаются автоматически
•	для визуализации метрик приложения реализован дашборд графаны 
Проверка
Для проверки запуска сервисов можно использовать:
http://178.154.222.123:8000/
